{
  "STATUS_REPORT": {
    "CONFIG_TYPE": "Enum",
    "ENUM_NAME": "StatusReport",
    "DESCRIPTION": "After a scrape job has successfully completed, you can display a report for the MLB season displaying various metrics for all data sets. The options below determine the level of detail reported.",
    "SAME_SETTING_FOR_ALL_DATA_SETS": true,
    "ALL": "SEASON_SUMMARY",
    "BBREF_GAMES_FOR_DATE": null,
    "BROOKS_GAMES_FOR_DATE": null,
    "BBREF_BOXSCORES": null,
    "BROOKS_PITCH_LOGS": null,
    "BROOKS_PITCHFX": null
  },
  "S3_BUCKET": {
    "CONFIG_TYPE": "str",
    "DESCRIPTION": "The name of the S3 bucket to use for storing HTML and/or JSON files",
    "SAME_SETTING_FOR_ALL_DATA_SETS": true,
    "ALL": "your-bucket",
    "BBREF_GAMES_FOR_DATE": null,
    "BROOKS_GAMES_FOR_DATE": null,
    "BBREF_BOXSCORES": null,
    "BROOKS_PITCH_LOGS": null,
    "BROOKS_PITCHFX": null
  },
  "SCRAPE_CONDITION": {
    "CONFIG_TYPE": "Enum",
    "ENUM_NAME": "ScrapeCondition",
    "DESCRIPTION": "By default, HTML is scraped and parsed only once (ONLY_MISSING_DATA). You can overwrite existing data by selecting ALWAYS, or prevent any data from being scraped by selecting NEVER.",
    "SAME_SETTING_FOR_ALL_DATA_SETS": true,
    "ALL": "ONLY_MISSING_DATA",
    "BBREF_GAMES_FOR_DATE": null,
    "BROOKS_GAMES_FOR_DATE": null,
    "BBREF_BOXSCORES": null,
    "BROOKS_PITCH_LOGS": null,
    "BROOKS_PITCHFX": null
  },
  "SCRAPE_TASK_OPTION": {
    "CONFIG_TYPE": "Enum",
    "ENUM_NAME": "ScrapeTaskOption",
    "DESCRIPTION": [
      [
        "A scrape job is created by choosing a start date, an end date and any data sets you wish to scrape for each day in the specified date range. When the job is ran, a list of dates is created from the start and end dates (both endpoints are included in the list).\n",
        "If SCRAPE_TASK_OPTION == BY_DATE (default behavior), the job is executed by iterating through the list of dates. For each data set a list of URLs to scrape for the current date is generated. After all data sets specified for this job have been scraped, the job moves to the next date in the list and scrapes all data sets for that date. After all days in the list have been scraped, the job ends.\n",
        "If SCRAPE_TASK_OPTION == BY_DATA_SET, the job is executed by iterating through the list of data sets to scrape. For each data set, a list of URLs to scrape for all days in the range specified by the start and end dates is generated. After all URLs for the data set have been scraped, the job moves on to the next data set and scrapes all URLs for the specified date range. After all data sets have been scraped, the job ends."
      ]
    ],
    "SAME_SETTING_FOR_ALL_DATA_SETS": true,
    "ALL": "BY_DATE",
    "BBREF_GAMES_FOR_DATE": null,
    "BROOKS_GAMES_FOR_DATE": null,
    "BBREF_BOXSCORES": null,
    "BROOKS_PITCH_LOGS": null,
    "BROOKS_PITCHFX": null
  },
  "URL_SCRAPE_DELAY": {
    "CONFIG_TYPE": "Numeric",
    "CLASS_NAME": "UrlScrapeDelay",
    "DESCRIPTION": "As a common courtesy (a.k.a, to avoid being banned), after scraping a webpage you should wait for a few seconds before requesting the next URL. You can specify a single length of time to use for all URLs, or create random delay lengths by specifying a minimum and maximum length of time.",
    "SAME_SETTING_FOR_ALL_DATA_SETS": true,
    "ALL": {
      "URL_SCRAPE_DELAY_IS_REQUIRED": true,
      "URL_SCRAPE_DELAY_IS_RANDOM": true,
      "URL_SCRAPE_DELAY_IN_SECONDS": null,
      "URL_SCRAPE_DELAY_IN_SECONDS_MIN": 3,
      "URL_SCRAPE_DELAY_IN_SECONDS_MAX": 6
    },
    "BBREF_GAMES_FOR_DATE": null,
    "BROOKS_GAMES_FOR_DATE": null,
    "BBREF_BOXSCORES": null,
    "BROOKS_PITCH_LOGS": null,
    "BROOKS_PITCHFX": null
  },
  "BATCH_JOB_SETTINGS": {
    "CONFIG_TYPE": "Numeric",
    "CLASS_NAME": "BatchJobSettings",
    "DESCRIPTION": "Number of URLs to scrape per batch. You can specify a single amount to use for all batches, or create random batch sizes by specifying a minimum and maximum batch size.",
    "SAME_SETTING_FOR_ALL_DATA_SETS": true,
    "ALL": {
      "CREATE_BATCHED_SCRAPE_JOBS": true,
      "USE_IRREGULAR_BATCH_SIZES": true,
      "BATCH_SIZE": null,
      "BATCH_SIZE_MIN": 50,
      "BATCH_SIZE_MAX": 80
    },
    "BBREF_GAMES_FOR_DATE": null,
    "BROOKS_GAMES_FOR_DATE": null,
    "BBREF_BOXSCORES": null,
    "BROOKS_PITCH_LOGS": null,
    "BROOKS_PITCHFX": null
  },
  "BATCH_SCRAPE_DELAY": {
    "CONFIG_TYPE": "Numeric",
    "CLASS_NAME": "BatchScrapeDelay",
    "DESCRIPTION": "Some websites will ban you even if you wait for a few seconds between each request. To avoid being banned, you can scrape URLs in batches (recommended batch size: ~50 URLs/batch) and wait for a long period of time (30-45 minutes) before you begin a new batch. You can specify a single length of time to use for all batches or create random delay lengths by specifying a minimum and maximum length of time.",
    "SAME_SETTING_FOR_ALL_DATA_SETS": false,
    "ALL": null,
    "BBREF_GAMES_FOR_DATE": {
      "BATCH_SCRAPE_DELAY_IS_REQUIRED": true,
      "BATCH_SCRAPE_DELAY_IS_RANDOM": true,
      "BATCH_SCRAPE_DELAY_IN_MINUTES": null,
      "BATCH_SCRAPE_DELAY_IN_MINUTES_MIN": 5,
      "BATCH_SCRAPE_DELAY_IN_MINUTES_MAX": 10
    },
    "BROOKS_GAMES_FOR_DATE": {
      "BATCH_SCRAPE_DELAY_IS_REQUIRED": true,
      "BATCH_SCRAPE_DELAY_IS_RANDOM": true,
      "BATCH_SCRAPE_DELAY_IN_MINUTES": null,
      "BATCH_SCRAPE_DELAY_IN_MINUTES_MIN": 30,
      "BATCH_SCRAPE_DELAY_IN_MINUTES_MAX": 45
    },
    "BBREF_BOXSCORES": {
      "BATCH_SCRAPE_DELAY_IS_REQUIRED": true,
      "BATCH_SCRAPE_DELAY_IS_RANDOM": true,
      "BATCH_SCRAPE_DELAY_IN_MINUTES": null,
      "BATCH_SCRAPE_DELAY_IN_MINUTES_MIN": 5,
      "BATCH_SCRAPE_DELAY_IN_MINUTES_MAX": 10
    },
    "BROOKS_PITCH_LOGS": {
      "BATCH_SCRAPE_DELAY_IS_REQUIRED": true,
      "BATCH_SCRAPE_DELAY_IS_RANDOM": true,
      "BATCH_SCRAPE_DELAY_IN_MINUTES": null,
      "BATCH_SCRAPE_DELAY_IN_MINUTES_MIN": 30,
      "BATCH_SCRAPE_DELAY_IN_MINUTES_MAX": 45
    },
    "BROOKS_PITCHFX": {
      "BATCH_SCRAPE_DELAY_IS_REQUIRED": true,
      "BATCH_SCRAPE_DELAY_IS_RANDOM": true,
      "BATCH_SCRAPE_DELAY_IN_MINUTES": null,
      "BATCH_SCRAPE_DELAY_IN_MINUTES_MIN": 30,
      "BATCH_SCRAPE_DELAY_IN_MINUTES_MAX": 45
    }
  },
  "HTML_STORAGE": {
    "CONFIG_TYPE": "Enum",
    "ENUM_NAME": "HtmlStorageOption",
    "DESCRIPTION": "By default, HTML is NOT saved after it has been parsed. However, you can choose to save scraped HTML in a local folder, S3 bucket, or both.",
    "SAME_SETTING_FOR_ALL_DATA_SETS": true,
    "ALL": "NONE",
    "BBREF_GAMES_FOR_DATE": null,
    "BROOKS_GAMES_FOR_DATE": null,
    "BBREF_BOXSCORES": null,
    "BROOKS_PITCH_LOGS": null,
    "BROOKS_PITCHFX": null
  },
  "HTML_LOCAL_FOLDER_PATH": {
    "CONFIG_TYPE": "Path",
    "CLASS_NAME": "LocalFolderPathSetting",
    "DESCRIPTION": "Local folder path where scraped HTML should be stored. The application will always check for saved HTML content in this location before sending a request to the website.",
    "SAME_SETTING_FOR_ALL_DATA_SETS": true,
    "ALL": "html_storage/{year}/{data_set}/",
    "BBREF_GAMES_FOR_DATE": null,
    "BROOKS_GAMES_FOR_DATE": null,
    "BBREF_BOXSCORES": null,
    "BROOKS_PITCH_LOGS": null,
    "BROOKS_PITCHFX": null
  },
  "HTML_S3_FOLDER_PATH": {
    "CONFIG_TYPE": "Path",
    "CLASS_NAME": "S3FolderPathSetting",
    "DESCRIPTION": "Path to a folder within an S3 bucket where scraped HTML should be stored. The application will always check for saved HTML content in this location before sending a request to the website.",
    "SAME_SETTING_FOR_ALL_DATA_SETS": true,
    "ALL": "{year}/{data_set}/html/",
    "BBREF_GAMES_FOR_DATE": null,
    "BROOKS_GAMES_FOR_DATE": null,
    "BBREF_BOXSCORES": null,
    "BROOKS_PITCH_LOGS": null,
    "BROOKS_PITCHFX": null
  },
  "JSON_STORAGE": {
    "CONFIG_TYPE": "Enum",
    "ENUM_NAME": "JsonStorageOption",
    "DESCRIPTION": "MLB data is parsed from HTML and stored in JSON docs. You can store the JSON docs in a local folder, S3 bucket, or both.",
    "SAME_SETTING_FOR_ALL_DATA_SETS": true,
    "ALL": "LOCAL_FOLDER",
    "BBREF_GAMES_FOR_DATE": null,
    "BROOKS_GAMES_FOR_DATE": null,
    "BBREF_BOXSCORES": null,
    "BROOKS_PITCH_LOGS": null,
    "BROOKS_PITCHFX": null
  },
  "JSON_LOCAL_FOLDER_PATH": {
    "CONFIG_TYPE": "Path",
    "CLASS_NAME": "LocalFolderPathSetting",
    "DESCRIPTION": "Local folder path where parsed JSON data should be stored.",
    "SAME_SETTING_FOR_ALL_DATA_SETS": true,
    "ALL": "json_storage/{year}/{data_set}/",
    "BBREF_GAMES_FOR_DATE": null,
    "BROOKS_GAMES_FOR_DATE": null,
    "BBREF_BOXSCORES": null,
    "BROOKS_PITCH_LOGS": null,
    "BROOKS_PITCHFX": null
  },
  "JSON_S3_FOLDER_PATH": {
    "CONFIG_TYPE": "Path",
    "CLASS_NAME": "S3FolderPathSetting",
    "DESCRIPTION": "Path to a folder within an S3 bucket where parsed JSON data should be stored.",
    "SAME_SETTING_FOR_ALL_DATA_SETS": true,
    "ALL": "{year}/{data_set}/",
    "BBREF_GAMES_FOR_DATE": null,
    "BROOKS_GAMES_FOR_DATE": null,
    "BBREF_BOXSCORES": null,
    "BROOKS_PITCH_LOGS": null,
    "BROOKS_PITCHFX": null
  },
  "SCRAPED_DATA_COMBINE_CONDITION": {
    "CONFIG_TYPE": "Enum",
    "ENUM_NAME": "ScrapeCondition",
    "DESCRIPTION": "By default, the process to combine and audit all scraped data for a single game is done only once (ONLY_MISSING_DATA). You can overwrite existing data by selecting ALWAYS, or prevent any data from being combined by selecting NEVER.",
    "SAME_SETTING_FOR_ALL_DATA_SETS": true,
    "ALL": "ONLY_MISSING_DATA",
    "BBREF_GAMES_FOR_DATE": null,
    "BROOKS_GAMES_FOR_DATE": null,
    "BBREF_BOXSCORES": null,
    "BROOKS_PITCH_LOGS": null,
    "BROOKS_PITCHFX": null
  },
  "COMBINED_DATA_STORAGE": {
    "CONFIG_TYPE": "Enum",
    "ENUM_NAME": "CombinedDataStorageOption",
    "DESCRIPTION": "When all data sets have been scraped for a single game, the parsed data is audited and combined into a single JSON file. You can choose to store these files on your local file system, on S3, or both.",
    "SAME_SETTING_FOR_ALL_DATA_SETS": true,
    "ALL": "LOCAL_FOLDER",
    "BBREF_GAMES_FOR_DATE": null,
    "BROOKS_GAMES_FOR_DATE": null,
    "BBREF_BOXSCORES": null,
    "BROOKS_PITCH_LOGS": null,
    "BROOKS_PITCHFX": null
  },
  "COMBINED_DATA_LOCAL_FOLDER_PATH": {
    "CONFIG_TYPE": "Path",
    "CLASS_NAME": "LocalFolderPathSetting",
    "DESCRIPTION": "Local folder path where files containing combined game data should be stored.",
    "SAME_SETTING_FOR_ALL_DATA_SETS": true,
    "ALL": "json_storage/{year}/combined_data",
    "BBREF_GAMES_FOR_DATE": null,
    "BROOKS_GAMES_FOR_DATE": null,
    "BBREF_BOXSCORES": null,
    "BROOKS_PITCH_LOGS": null,
    "BROOKS_PITCHFX": null
  },
  "COMBINED_DATA_S3_FOLDER_PATH": {
    "CONFIG_TYPE": "Path",
    "CLASS_NAME": "S3FolderPathSetting",
    "DESCRIPTION": "Path to a folder within an S3 bucket where files containing combined game data should be stored.",
    "SAME_SETTING_FOR_ALL_DATA_SETS": true,
    "ALL": "{year}/combined_data",
    "BBREF_GAMES_FOR_DATE": null,
    "BROOKS_GAMES_FOR_DATE": null,
    "BBREF_BOXSCORES": null,
    "BROOKS_PITCH_LOGS": null,
    "BROOKS_PITCHFX": null
  },
  "DB_BACKUP_FOLDER_PATH": {
    "CONFIG_TYPE": "Path",
    "CLASS_NAME": "LocalFolderPathSetting",
    "DESCRIPTION": "Local folder path where csv files containing exported table data should be stored.",
    "SAME_SETTING_FOR_ALL_DATA_SETS": true,
    "ALL": "backup",
    "BBREF_GAMES_FOR_DATE": null,
    "BROOKS_GAMES_FOR_DATE": null,
    "BBREF_BOXSCORES": null,
    "BROOKS_PITCH_LOGS": null,
    "BROOKS_PITCHFX": null
  }
}